{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl_Vh2LSPP6X"
      },
      "source": [
        "# Emotion Recognition - Assignment 2\n",
        "\n",
        "Names:\n",
        "*   Alejandro Eisen Jofré\n",
        "*   Noor Chaloner Yassein\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_1Sv6Cy1JwU"
      },
      "source": [
        "# Overview\n",
        "\n",
        "In this project, four main models (plus various configurations) were used in an attempt to find the best model for classifying emotions within images. The model that performed the best was the convolutional neural network (CNN), and was thus the final recommended model for this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjyBJqTb1QrE"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "Exploring the data found a limited number of images, and particularly low numbers for certain emotions classes. Further, the data was given in one-channel pixels (i.e: grayscale), ranging from 0-255, and as such had to be transformed and scaled down to a 0-1 range to pass it into the neural network models. For this we simply divided the pixels by 255.\n",
        "\n",
        "Given the low number of images, data augmentation was applied based off ImageDataGenerator from Keras, which was passed into the models. The script works by iterating over the original data and performing alterations of each of the images within the model as it ran, allowing the model to train on shifted, rotated, and zoomed versions of the original images. Data augmentation was passed only onto our training data, and as such the validation was performed with the “original” images.\n",
        "\n",
        "There was a test and training set given, so the data did not need to be split for that purpose. However, the training and validation strategy was to split the data in the training set so that all the training data would be up to row 2900, and the data for validation would be all the rows after row 2900, which was about 10% of the training data. This was justified by a study on validation splits using a similarly limited dataset, which found 90/10 to be the split giving the highest accuracy (Muraina, 2022).\n",
        "\n",
        "Learning rates were another decision that had to be made for each of the models. The main learning rates were the 1cycle policy, one which decreases exponentially throughout the model, and a plateau one. The 1cycle policy class allows the learning rate to start at a specific point, then increase up to its maximum, and decrease to a minimum lower than the original specific point. This increases accuracy, as a number of learning rates can be attempted before settling on a specific one (Smith, 2018). The exponentially decreasing learning rate is useful as it balances the benefits of both a large and small learning rate - essentially, the steps start out large, allowing efficient progress to be made, and gradually become smaller, which allows the model to converge (Géron, 2019, p. 183). The plateau one reduces the learning rate when in a plateau of loss for a defined number of epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKhXocJEBjEo"
      },
      "source": [
        "## Basic Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw8GnCbnBTPd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, datetime\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from google.colab import drive\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca0VjULOBZCr",
        "outputId": "92bc0d91-d479-429e-a3a0-e9cd140d3991"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljXTYTmIUAX1",
        "outputId": "eebd1941-dd9c-4b5c-ad9d-b67fc1283191"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/CS985/cs985-987-emotion-recognition-project-2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jchDNqUjBqhN"
      },
      "source": [
        "## Data Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1_-jW95BqET"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"my_emotion_train.csv\")\n",
        "df_test = pd.read_csv(\"my_emotion_test.csv\")\n",
        "\n",
        "data_train = df_train.copy() #copies of data for rollback just in case\n",
        "data_test = df_test.copy()\n",
        "\n",
        "data_train['pixels'] = data_train['pixels'].apply(lambda x: [int(pixel) for pixel in x.split()]) #Convesion of pixel column from str to list of int separated by ,\n",
        "data_test['pixels'] = data_test['pixels'].apply(lambda x: [int(pixel) for pixel in x.split()])\n",
        "\n",
        "#check for empty rows or the ones containing missing values\n",
        "i = 0\n",
        "while i in range(len(data_train)):\n",
        "    if len(data_train.pixels[i])==0:\n",
        "        print(f'empty here: {i}')\n",
        "    elif np.nan in data_train.pixels[i]:\n",
        "        print(f'empty here: {i}')\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XD5ogwDCXY9"
      },
      "outputs": [],
      "source": [
        "def pixel_division(pixel_list):\n",
        "  return[pixel / 255.0 for pixel in pixel_list] #scaling data into valid 0-1 range for neural networks\n",
        "\n",
        "data_train['pixels'] = data_train['pixels'].apply(pixel_division)\n",
        "data_test['pixels'] = data_test['pixels'].apply(pixel_division)\n",
        "\n",
        "X_train_full, y_train_full, id = data_train.pixels, data_train.emotion, data_train.id\n",
        "\n",
        "\n",
        "X_valid, X_train = X_train_full[:2900], X_train_full[2900:] #separate data into validation and training (10% aproximately)\n",
        "y_valid, y_train = y_train_full[:2900], y_train_full[2900:]\n",
        "\n",
        "X_valid = np.array(X_valid.to_list()) #convert to array\n",
        "X_train = np.array(X_train.to_list())\n",
        "y_valid = np.array(y_valid.to_list())\n",
        "y_train = np.array(y_train.to_list())\n",
        "\n",
        "X_valid = X_valid.reshape(-1, 48, 48, 1) #reshape to pass into neural network\n",
        "X_train = X_train.reshape(-1, 48, 48, 1)\n",
        "\n",
        "X_test = data_test['pixels']\n",
        "X_test = np.array(X_test.to_list())\n",
        "X_test_id = data_test[\"id\"]\n",
        "X_test = X_test.reshape(-1, 48, 48, 1)  # Reshape to match model's expected input shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04X4sH3QCPoh"
      },
      "source": [
        "## Class (im)balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-yD1juLCNxo",
        "outputId": "f42b81a8-efad-4ac0-87d2-42ebcd51d0ef"
      },
      "outputs": [],
      "source": [
        "emotion_names = {\n",
        "    0: \"Angry\",\n",
        "    1: \"Disgust\",\n",
        "    2: \"Fear\",\n",
        "    3: \"Happy\",\n",
        "    4: \"Sad\",\n",
        "    5: \"Surprise\",\n",
        "    6: \"Neutral\"\n",
        "}\n",
        "\n",
        "emotion_counts = data_train[\"emotion\"].value_counts()\n",
        "emotion_counts = emotion_counts.rename(emotion_names)\n",
        "emotion_counts.plot(kind=\"bar\") #we can see the imbalanced distribution of classes here. let's fix it.\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(class_weight = 'balanced',\n",
        "                                                  classes= np.unique(y_train),\n",
        "                                                  y = y_train)\n",
        "class_weight_dict = dict(enumerate(class_weights)) #we can use this later to pass as an argument into our networks as class weights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzO90uEnDHx4"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1pWsRo3DJg7"
      },
      "outputs": [],
      "source": [
        "data_generation = ImageDataGenerator( #check hyperparameters to tune properly\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    featurewise_center=False,\n",
        "    samplewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    samplewise_std_normalization=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXcTWIIwDM2i"
      },
      "source": [
        "## Standard Machine Learning (ML) Model\n",
        "\n",
        "For the standard ML model, a basic stochastic gradient descent (SGD) model was chosen. This decision was made as it is a basis for early neural network models, and as such, was a good place to start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY6-4cRVDO_u",
        "outputId": "e893431d-28a4-4edb-ac09-e73c81e0f8d5"
      },
      "outputs": [],
      "source": [
        "X_train_full_array = np.array(X_train_full.to_list()) #since we dont need the validation data for this, we use the full X_train\n",
        "X_train_full_array\n",
        "sgd_X_train, sgd_X_test = train_test_split(X_train_full_array, test_size = 0.2)\n",
        "sgd_y_train, sgd_y_test = train_test_split(y_train_full, test_size = 0.2)\n",
        "\n",
        "#print(sgd_X_train.shape, sgd_X_test.shape, sgd_y_train.shape, sgd_y_test.shape)\n",
        "\n",
        "sgd_clf = SGDClassifier()\n",
        "sgd_clf.fit(sgd_X_train, sgd_y_train)\n",
        "\n",
        "sgd_predictions = sgd_clf.predict(sgd_X_test)\n",
        "sgd_conf_mat = confusion_matrix(sgd_y_test, sgd_predictions)\n",
        "sgd_acc = accuracy_score(sgd_y_test, sgd_predictions)\n",
        "print(sgd_conf_mat, sgd_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbECWmkBOY82",
        "outputId": "d82f8440-f286-4458-9325-7a3ff54dfef6"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBgRedDsC5mb"
      },
      "outputs": [],
      "source": [
        "sgd_xtest = data_test['pixels']\n",
        "sgd_xtest = np.array(sgd_xtest.to_list())\n",
        "sgd_ytest_pred = sgd_clf.predict(sgd_xtest)\n",
        "\n",
        "sgd_data = {'id': X_test_id, 'predicted class': sgd_ytest_pred}\n",
        "sgd_data = pd.DataFrame(sgd_data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "sgd_data.to_csv(\"sgd_pred.csv\", index=False)  # Set index=False to avoid saving the index column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1vJsWlbDqNW"
      },
      "source": [
        "## Deep Model Configurations\n",
        "\n",
        "\n",
        "The first deep model configuration had five layers, decreasing neurons, and a regular decreasing learning rate. Given the significant time of convergence, the next configuration included a dropout layer for regularisation. This also increased the already-augmented data, as dropout layers drop a certain percentage of neurons in each iteration to vary the training set (Géron, 2019, p. 479). However, the accuracy was lower than the first model, thus l1 regularisation was implemented next instead of dropout. This iteration also performed worse, and so a batch normalisation layer was added in the next configuration, to normalise and standardise the training set, and help with accuracy. This performed better, but still not as high as the first, so l2 regularisation was attempted next - first with a dropout layer, as research suggested that this combination was useful (Srivastava et al, 2014). It did not prove useful, however, so the next configuration implemented batch normalisation instead. The accuracy scores improved significantly in this configuration. The combination of l1&l2 regularisation was next, but both together was not found to be an improvement, so l2 regularisation was settled on. For the final model, the learning rate was changed to the 1cycle class (created below), to see impacts of a different learning rate. The results did not improve, and thus the best configuration was the fifth, with l2 regularization & batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_swphzuD1aG"
      },
      "outputs": [],
      "source": [
        "class DeepModel(keras.models.Model):\n",
        "    def __init__(self, units=500, activation='relu', kernel_initializer='he_normal', #he_normal initialization for ReLU and others from that family\n",
        "                 hidden_layers=5, use_batch_norm = False, momentum = 0.99,\n",
        "                 dropout_rate = 0.0, kernel_regularizer = None, last_layer_large=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden_layers = []\n",
        "        self.flatten = keras.layers.Flatten(input_shape=[48, 48])\n",
        "        if last_layer_large:\n",
        "          for i in range(hidden_layers-1):\n",
        "            self.hidden_layers.append(keras.layers.Dense(units - i * 100,\n",
        "                                                         activation = activation,\n",
        "                                                         kernel_initializer=kernel_initializer,\n",
        "                                                         kernel_regularizer = self.regularizer(kernel_regularizer)))\n",
        "          self.hidden_layers.append(keras.layers.Dense(units = 350,\n",
        "                                      activation = activation,\n",
        "                                      kernel_initializer=kernel_initializer,\n",
        "                                      kernel_regularizer = self.regularizer(kernel_regularizer)))\n",
        "        else:\n",
        "          self.hidden_layers = [\n",
        "              keras.layers.Dense(units - i * 100,\n",
        "                                activation=activation,\n",
        "                                kernel_initializer=kernel_initializer,\n",
        "                                kernel_regularizer=self.regularizer(kernel_regularizer))\n",
        "              for i in range(hidden_layers)\n",
        "            ]\n",
        "        if use_batch_norm == True:\n",
        "          self.batch_normalization_layers = [\n",
        "              keras.layers.BatchNormalization(momentum = momentum) for _ in range(hidden_layers) #keep momentum close to 1. 0.99 default. can use higher (0.999, 0.9999, etc) for larger datasets.\n",
        "          ]\n",
        "        else:\n",
        "          self.batch_normalization_layers = None\n",
        "\n",
        "        if dropout_rate > 0.0:\n",
        "          self.dropout_layers = [\n",
        "              keras.layers.Dropout(rate = dropout_rate) for _ in range(hidden_layers) #can specify dropout rate\n",
        "          ]\n",
        "        else:\n",
        "          self.dropout_layers = None\n",
        "        self.output_layer = keras.layers.Dense(7, activation=\"softmax\", kernel_initializer = \"glorot_uniform\") # glorot for softmax according to hands-on book p.448\n",
        "        #we usually skip regularizing in output layer\n",
        "\n",
        "    def regularizer(self, kernel_regularizer):\n",
        "      if kernel_regularizer == None:\n",
        "        return None\n",
        "      elif kernel_regularizer == \"l1\":\n",
        "        return keras.regularizers.L1()\n",
        "      elif kernel_regularizer == \"l2\":\n",
        "        return keras.regularizers.L2()\n",
        "      elif kernel_regularizer == \"l1l2\":\n",
        "        return keras.regularizers.L1L2()\n",
        "      else:\n",
        "        return ValueError(\"Not a valid regularizer. Use 'l1', 'l2' or 'l1l2'\")\n",
        "\n",
        "    def call(self, inputs, training = None):\n",
        "        x = self.flatten(inputs)\n",
        "        for i, layer in enumerate(self.hidden_layers):\n",
        "          x = layer(x)\n",
        "          if self.batch_normalization_layers is not None:\n",
        "            x = self.batch_normalization_layers[i](x, training = training)\n",
        "          if self.dropout_layers is not None:\n",
        "            x = self.dropout_layers[i](x, training = training)\n",
        "        output = self.output_layer(x)\n",
        "        return output\n",
        "\n",
        "class OneCycleScheduler(keras.callbacks.Callback):\n",
        "    def __init__(self, iterations, max_rate, start_rate=None,\n",
        "                 last_iterations=None, last_rate=None):\n",
        "        self.iterations = iterations\n",
        "        self.max_rate = max_rate\n",
        "        self.start_rate = start_rate or max_rate / 10\n",
        "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
        "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
        "        self.last_rate = last_rate or self.start_rate / 1000\n",
        "        self.iteration = 0\n",
        "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
        "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
        "                / (iter2 - iter1) + rate1)\n",
        "    def on_batch_begin(self, batch, logs):\n",
        "        if self.iteration < self.half_iteration:\n",
        "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
        "        elif self.iteration < 2 * self.half_iteration:\n",
        "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
        "                                     self.max_rate, self.start_rate)\n",
        "        else:\n",
        "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
        "                                     self.start_rate, self.last_rate)\n",
        "        self.iteration += 1\n",
        "        K.set_value(self.model.optimizer.lr, rate)\n",
        "\n",
        "\n",
        "'''Checkpoints'''\n",
        "annealer_exp = LearningRateScheduler(lambda x: 1e-4 * 0.95 ** x)\n",
        "\n",
        "annealer_1cycle = OneCycleScheduler(iterations = 81600, #approximate of epochs*steps per epoch = 100 * 816\n",
        "                                    max_rate = 0.01,\n",
        "                                    start_rate = 0.001,\n",
        "                                    last_iterations = 9063,\n",
        "                                    last_rate = 1e-7)\n",
        "\n",
        "annealer_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5)\n",
        "\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20, #patience is the nº of epochs where if no improvement, stops\n",
        "                                                  restore_best_weights=True)\n",
        "'''Optimizer'''\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvu_D9HKEqiD"
      },
      "source": [
        "### Configuration 1 - baseline deep model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlMRSIn7EZq8",
        "outputId": "65be45db-d8ab-49ca-cd2a-4695cea74d81"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_1\", save_best_only=True)\n",
        "\n",
        "dnn_1 = DeepModel(use_batch_norm= False)\n",
        "dnn_1.compile(optimizer=opt, #perform backpropagation with adam\n",
        "              loss = \"sparse_categorical_crossentropy\", # we use this loss because we have sparse labels (each instance there is just a target class index from 0-6 and classes are exclusive) and not vectors in which case we'd use categorical_crossentropy\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_1 = dnn_1.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, early_stopping_cb, annealer_exp])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TCVXJ5NExbj"
      },
      "source": [
        "### Configuration 2: dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XocxpmmLExbk",
        "outputId": "6f0704f4-3858-408e-eebd-6b09e8ac0ad5"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_2\", save_best_only=True)\n",
        "\n",
        "dnn_2 = DeepModel(activation=\"relu\", use_batch_norm= False, dropout_rate=0.3)\n",
        "dnn_2.compile(optimizer=opt,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_2 = dnn_2.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, early_stopping_cb, annealer_exp])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3KeRt9iE1ws"
      },
      "source": [
        "### Configuration 3: l1 regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1GkjZrVE1ws",
        "outputId": "3fe3095a-f151-484d-ac84-ef5e854e3fc7"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_3\", save_best_only=True)\n",
        "\n",
        "dnn_3 = DeepModel(activation=\"relu\", use_batch_norm= False, kernel_regularizer=\"l1\")\n",
        "dnn_3.compile(optimizer=opt,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_3 = dnn_3.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, early_stopping_cb, annealer_exp])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvVPCBU8E2eX"
      },
      "source": [
        "### Configuration 4: l1 regularization & batch norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUY5neoqE2eX",
        "outputId": "6e08f38e-085d-465b-bfab-c4139133ba26"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_4\", save_best_only=True)\n",
        "\n",
        "dnn_4 = DeepModel(activation=\"relu\", use_batch_norm= True, momentum = 0.9, kernel_regularizer=\"l1\")\n",
        "dnn_4.compile(optimizer=opt,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_4 = dnn_4.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, early_stopping_cb, annealer_exp])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GBr7a7UE3fx"
      },
      "source": [
        "### Configuration 5: l2 regularization & batch norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaK9Tb4xE3fy",
        "outputId": "b7d4e06b-d05f-4183-8776-e81df5b1106c"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_5\", save_best_only=True)\n",
        "\n",
        "dnn_5 = DeepModel(activation=\"relu\", use_batch_norm= True, momentum = 0.9, kernel_regularizer=\"l2\")\n",
        "dnn_5.compile(optimizer=opt,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_5 = dnn_5.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, early_stopping_cb, annealer_exp])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrXtGnI_E38i"
      },
      "source": [
        "### Configuration 6: l1|2 regularization & batch norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bDqKEKDE38j",
        "outputId": "f42e9ae4-06a2-4956-81ca-8a2aef721dfe"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_6\", save_best_only=True)\n",
        "\n",
        "dnn_6 = DeepModel(activation=\"relu\", use_batch_norm= True, momentum = 0.9, kernel_regularizer=\"l1l2\")\n",
        "dnn_6.compile(optimizer=opt,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_6 = dnn_6.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, early_stopping_cb, annealer_exp])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uilv6JICE4WD"
      },
      "source": [
        "### Configuration 7: 1cycle & l2 regularization & batch norm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-qd-z-uE4WE",
        "outputId": "501dcb9e-0063-4c82-b411-d9f5d55dbb41"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_7\", save_best_only=True)\n",
        "\n",
        "dnn_7 = DeepModel(use_batch_norm= True, momentum = 0.9, kernel_regularizer=\"l2\")\n",
        "dnn_7.compile(optimizer=opt,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_7 = dnn_7.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, annealer_1cycle])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic2Y4IlePCoi"
      },
      "source": [
        "### Configuration 1b - baseline deep model (different random seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7Lt8MugPCoi"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(43)\n",
        "tf.random.set_seed(43)\n",
        "\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_1b\", save_best_only=True)\n",
        "\n",
        "dnn_1b = DeepModel(use_batch_norm= False)\n",
        "dnn_1b.compile(optimizer=opt, #perform backpropagation with adam\n",
        "              loss = \"sparse_categorical_crossentropy\", # we use this loss because we have sparse labels (each instance there is just a target class index from 0-6 and classes are exclusive) and not vectors in which case we'd use categorical_crossentropy\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_1b = dnn_1b.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, early_stopping_cb, annealer_exp])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQyryjYaPCoj"
      },
      "source": [
        "### Configuration 2b: dropout (different random seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd8kSYb5PCoj"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(43)\n",
        "tf.random.set_seed(43)\n",
        "\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_2b\", save_best_only=True)\n",
        "\n",
        "dnn_2b = DeepModel(activation=\"relu\", use_batch_norm= False, dropout_rate=0.3)\n",
        "dnn_2b.compile(optimizer=opt,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_2b = dnn_2b.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, early_stopping_cb, annealer_exp])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgZGOGqUPCoj"
      },
      "source": [
        "### Configuration 3b: l1 regularization (different random seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT8dUhojPCok"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(43)\n",
        "tf.random.set_seed(43)\n",
        "\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_3b\", save_best_only=True)\n",
        "\n",
        "dnn_3b = DeepModel(activation=\"relu\", use_batch_norm= False, kernel_regularizer=\"l1\")\n",
        "dnn_3b.compile(optimizer=opt,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_3b = dnn_3b.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, early_stopping_cb, annealer_exp])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWRVOBNQPCok"
      },
      "source": [
        "### Configuration 4b: l1 regularization & batch norm (different random seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR6L23zKPCok"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(43)\n",
        "tf.random.set_seed(43)\n",
        "\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_4b\", save_best_only=True)\n",
        "\n",
        "dnn_4b = DeepModel(activation=\"relu\", use_batch_norm= True, momentum = 0.9, kernel_regularizer=\"l1\")\n",
        "dnn_4b.compile(optimizer=opt,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_4b = dnn_4b.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, early_stopping_cb, annealer_exp])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPCy3HjmPCok"
      },
      "source": [
        "### Configuration 5b: l2 regularization & batch norm (different random seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWUT-x-TPCol",
        "outputId": "2d2a4327-27c9-44af-ebcc-ff4189027da4"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(43)\n",
        "tf.random.set_seed(43)\n",
        "\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_5b\", save_best_only=True)\n",
        "\n",
        "dnn_5b = DeepModel(activation=\"relu\", use_batch_norm= True, momentum = 0.9, kernel_regularizer=\"l2\")\n",
        "dnn_5b.compile(optimizer=opt,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_5b = dnn_5b.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, early_stopping_cb, annealer_exp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0oA-afpDyYs",
        "outputId": "b6f10cea-b7ca-4524-8c07-e7289473d39b"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "X_test = data_test['pixels']\n",
        "X_test = np.array(X_test.to_list())\n",
        "X_test_id = data_test[\"id\"]\n",
        "X_test = X_test.reshape(-1, 48, 48, 1)  # Reshape to match model's expected input shape\n",
        "\n",
        "dnn_best_model = load_model(\"dnn_config_5b\")\n",
        "dnn_predictions = dnn_best_model.predict(X_test)\n",
        "# Get predicted class indices for each image\n",
        "dnn_predicted_classes = np.argmax(dnn_predictions, axis=1)\n",
        "\n",
        "dnn_data = {'id': X_test_id, 'predicted class': dnn_predicted_classes}\n",
        "dnn_gru = pd.DataFrame(dnn_data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "dnn_gru.to_csv(\"dnn_pred.csv\", index=False)  # Set index=False to avoid saving the index column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74oD5PxbPCol"
      },
      "source": [
        "### Configuration 6b: l1|2 regularization & batch norm (different random seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoVpfc21PCol"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(43)\n",
        "tf.random.set_seed(43)\n",
        "\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_6b\", save_best_only=True)\n",
        "\n",
        "dnn_6b = DeepModel(activation=\"relu\", use_batch_norm= True, momentum = 0.9, kernel_regularizer=\"l1l2\")\n",
        "dnn_6b.compile(optimizer=opt,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_6b = dnn_6b.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, early_stopping_cb, annealer_exp])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY6V3wVcPCom"
      },
      "source": [
        "### Configuration 7b: 1cycle & l2 regularization & batch norm (different random seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSrq44fRPCom"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(43)\n",
        "tf.random.set_seed(43)\n",
        "\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_config_7b\", save_best_only=True)\n",
        "\n",
        "dnn_7b = DeepModel(use_batch_norm= True, momentum = 0.9, kernel_regularizer=\"l2\")\n",
        "dnn_7b.compile(optimizer=opt,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "history_7b = dnn_7b.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = X_train.shape[0]//32,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              callbacks = [checkpoint_cb, annealer_1cycle])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-fmeAHv40Lt"
      },
      "source": [
        "## GRU model\n",
        "\n",
        "For the second complex model, a Gated Recurrent Unit (GRU) model was chosen. A paper on the subject suggested the use of a GRU model combined with an attention mechanism to specify which sections of an image the model should be focusing on (Li et al, 2021). The idea was that, since classic CNNs do not assign discriminative weights to the informative local areas, this lack of focus may lead to misclassifications (Li et al, 2021). As such, a GRU was created, along with a custom multi-attention function which was added to the model as an individual layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVYaonZv44vs"
      },
      "outputs": [],
      "source": [
        "#create a class for Attention layer\n",
        "\n",
        "class Attention_function(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Attention_function, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "def call(self, features, hidden):\n",
        "  hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "  score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "  attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "  context_vector = attention_weights * features\n",
        "  context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "  return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRN-le0l5DTM"
      },
      "outputs": [],
      "source": [
        "#create the model\n",
        "\n",
        "gru_model = keras.models.Sequential()\n",
        "gru_model.add(keras.layers.Reshape((48, 48), input_shape=(48,48,1)))\n",
        "gru_model.add(keras.layers.GRU(32, activation='selu', kernel_initializer=\"lecun_normal\", input_shape = (48, 48, 1), return_sequences=True))\n",
        "gru_model.add(keras.layers.GRU(128, activation='selu', kernel_initializer=\"lecun_normal\"))\n",
        "gru_model.add(keras.layers.BatchNormalization())\n",
        "gru_model.add(Attention_function(160))\n",
        "gru_model.add(keras.layers.Dense(128))\n",
        "gru_model.add(keras.layers.BatchNormalization())\n",
        "gru_model.add(keras.layers.Dense(128))\n",
        "gru_model.add(keras.layers.Dense(160))\n",
        "gru_model.add(keras.layers.BatchNormalization())\n",
        "gru_model.add(keras.layers.Dense(7, activation = \"softmax\", kernel_initializer = \"glorot_normal\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF3LqHRj5FlV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "initial_lr = 1e-4\n",
        "def step_decay(epoch):\n",
        "    initial_lr = 1e-4\n",
        "    drop = 0.5\n",
        "    epochs_drop = 10\n",
        "    lr = initial_lr * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "    return lr\n",
        "\n",
        "annealer = LearningRateScheduler(step_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZT85d0m5HRn"
      },
      "outputs": [],
      "source": [
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"gru_model\", save_best_only=True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "\n",
        "\n",
        "#compile the model\n",
        "gru_model.compile(optimizer=opt,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "#fit the model\n",
        "gru_model = gru_model.fit(data_generation.flow(X_train, y_train, batch_size=32),\n",
        "                            epochs = 100,\n",
        "                            steps_per_epoch = X_train.shape[0]//32,\n",
        "                            validation_data = (X_valid, y_valid),\n",
        "                            callbacks = [checkpoint_cb, early_stopping_cb, annealer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyOqq4yo5XXP"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(gru_model.history).plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzdpioFF5WaU"
      },
      "outputs": [],
      "source": [
        "gru_best_model = load_model(\"gru_model\")\n",
        "gru_predictions = gru_best_model.predict(X_test)\n",
        "# Get predicted class indices for each image\n",
        "gru_predicted_classes = np.argmax(gru_predictions, axis=1)\n",
        "\n",
        "gru_data = {'id': X_test_id, 'predicted class': gru_predicted_classes}\n",
        "df_gru = pd.DataFrame(gru_data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_gru.to_csv(\"gru_pred.csv\", index=False)  # Set index=False to avoid saving the index column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUrH9QbLGMDk"
      },
      "source": [
        "## Convolutional Neural Network\n",
        "\n",
        "For the second (and final) complex model we chose a Convolutional Neural Network (CNN), this because CNNs are particularly good at image recognition tasks (Chauhan, Ghanshala, & Joshi, 2018). We tried a series of different configurations for our CNN architecture but all followed the same basic pattern:\n",
        "\n",
        "\n",
        "\n",
        "1.   Input\n",
        "2.   Convolution\n",
        "        - Conv2D (x1 or x2):\n",
        "            - Filters = 64, 128 or 256. Increasing depending on which convolution.\n",
        "            - Size = (7,7), (5,5) or (3,3). Decreasing in size depending on convolution\n",
        "            - Padding = Same\n",
        "            - Activation = Softplus\n",
        "            - Initializer = he_normal\n",
        "3. Pooling\n",
        "  - Strides = (2,2)\n",
        "  - Pool Size = (2,2)\n",
        "4. Batch Normalization\n",
        "5. Dropout\n",
        "6. Fully Connected\n",
        "7. Output\n",
        "\n",
        "We tried adding extra convolutions, changing the normalization momentum, the dropout rate, as well as batch sizes, and even the ImageDataGenerator parameters, some of which were trial and error decisions and others were due to previously known well performing configurations of CNNs. For example, both the AlexNet and LeNet-5 architectures show the use of fully connected layers right before the output. AlexNet model shows the use of more than one convolutional layer on top of one another without pooling in the middle (Géron, 2019). We used Softplus which is part of the ReLU family and seemed to perform better on our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Su607zcGKx0",
        "outputId": "9c085ea3-9e1f-40bf-ad38-59480ff8dbf4"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(43)\n",
        "tf.random.set_seed(43)\n",
        "\n",
        "cnn_model = Sequential([\n",
        "    Input(shape=(48,48,1)),\n",
        "    Conv2D(64, kernel_size=(7,7), activation='softplus', kernel_initializer='he_normal', padding = \"same\"),\n",
        "    Conv2D(64, kernel_size=(3,3), activation='softplus', kernel_initializer='he_normal', padding = \"same\"),\n",
        "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Conv2D(128, kernel_size=(3,3), activation='softplus', kernel_initializer='he_normal', padding = \"same\"),\n",
        "    Conv2D(128, kernel_size=(3,3), activation='softplus', kernel_initializer='he_normal', padding = \"same\"),\n",
        "    AveragePooling2D(pool_size=(2,2), strides=(2,2)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Conv2D(256, kernel_size=(3,3), activation='softplus',kernel_initializer='he_normal', padding = \"same\"),\n",
        "    Conv2D(256, kernel_size=(3,3), activation='softplus',kernel_initializer='he_normal', padding = \"same\"),\n",
        "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Conv2D(256, kernel_size=(3,3), activation='softplus',kernel_initializer='he_normal', padding = \"same\"),\n",
        "    Conv2D(256, kernel_size=(3,3), activation='softplus',kernel_initializer='he_normal', padding = \"same\"),\n",
        "    Flatten(),\n",
        "    Dense(500, activation='relu',kernel_initializer='he_normal'),\n",
        "    Dense(400, activation='relu',kernel_initializer='he_normal'),\n",
        "    Dense(300, activation='relu',kernel_initializer='he_normal'),\n",
        "    Dense(7, activation = \"softmax\", kernel_initializer = \"glorot_normal\")\n",
        "])\n",
        "\n",
        "#keras.utils.plot_model(cnn_model)\n",
        "opt = keras.optimizers.Adam(amsgrad=True)\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"cnn_model.keras\", save_best_only=True)\n",
        "\n",
        "total_samples = X_train.shape[0]\n",
        "batch_size = 64\n",
        "steps_per_epoch = total_samples // batch_size\n",
        "\n",
        "if total_samples % batch_size != 0:\n",
        "    steps_per_epoch += 1\n",
        "\n",
        "cnn_model.compile(optimizer=opt,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "cnn_model = cnn_model.fit(data_generation.flow(X_train, y_train, batch_size=batch_size, shuffle = True) ,\n",
        "                                   epochs=200,\n",
        "                                   steps_per_epoch=steps_per_epoch,\n",
        "                                   class_weight=class_weight_dict,\n",
        "                                   validation_data=(X_valid, y_valid),\n",
        "                                   callbacks=[checkpoint_cb, annealer_plateau, early_stopping_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMYqy8FoQKqX",
        "outputId": "0422f493-a8ed-4f4d-a65e-a697dc2cdb4d"
      },
      "outputs": [],
      "source": [
        "best_cnn = load_model(\"cnn_model.keras\")  # Load the best saved model\n",
        "cnn_predictions = best_cnn.predict(X_test)\n",
        "cnn_predicted_classes = np.argmax(cnn_predictions, axis=1)\n",
        "\n",
        "cnn_data = {'id': X_test_id, 'predicted class': cnn_predicted_classes}\n",
        "df_cnn = pd.DataFrame(cnn_data)\n",
        "\n",
        "\n",
        "df_cnn.to_csv(\"cnn_pred.csv\", index=False)  # index=False due to own index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AUGae046S0w"
      },
      "source": [
        "## Results\n",
        "\n",
        "The results table below outlines the training and testing scores for each model attempted. The first model was the base-level SGD model. Its performance wasn’t extremely high, with a mean of 0.2079 accuracy on the training data, however this served as a basis for the models that followed. The next model attempted was the deep neural network model, along with several configurations. Its performance was improved, as can be seen by the accuracy score in the table. Then a different configuration of the model was attempted – a single layer, which in theory is the optimal number for hidden layers (Heaton, 2008). However, the performance did not improve as expected. One potential reason for this could be that the shape of the dataset was not as expected – one hidden layer is best for continuous mapping, but two hidden layers are useful for functions with any shape (Heaton, 2008). The multiattention gated recurrent unit (GRU) model was also attempted. As explained above, using the multiattention function in combination with the GRU was meant to increase focus on the important aspects of the images, so as to increase accuracy and minimise the likelihood of misclassification. However, despite various iterations of the model (including changing the optimiser and number of layers, as well as varying the neurons in each layer), it did not perform as well as expected. One possible reason for this could be that the images were not high quality enough for the attention mechanism to be useful, as it was suggested in the paper that the highest impacts will likely be seen on high-quality images (Li et al, 2021). Finally, a convolutional neural network (CNN) was tried, with much improved performance. The Kaggle score of 0.59 (and mean accuracy score of 0.58) was the highest out of all the models attempted, and therefore was the chosen final recommendation for this task.\n",
        "\n",
        "Generally, increasing the number of layers tended to improve performance. Increasing the number of epochs also helped, as it allowed the model more time to learn. Data augmentation improved the performance for similar reasons. Additionally, in most models, batch normalization was useful – and generally should not be used next to dropout, so that was something to ensure throughout the process (Kim, 2021). However, we did find that adding both in our CNN gave us the best performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYZS9d1c8LLe"
      },
      "source": [
        "\n",
        "Model name  | Iteration | Accuracy Score - Training | Accuracy Score - Validation | Mean | Standard Deviation\n",
        "-------------------|------------------|----------------|----------|-------|----|\n",
        "Standard Baseline | 01 | 0.1934 | - | - |\n",
        " | 02 | 0.2181| - | - |\n",
        "- | 03 | 0.2112 | - | - |\n",
        " - | - | - | - | 0.2079 | 0.009 |\n",
        "| - |  |  |  |  |\n",
        "Deep Model    | 01 | 0.3949 | 0.4179 | - |\n",
        " | 02 | 0.3991 | 0.4186 | - |\n",
        " | - | - | - | 0.4076 | 0.0107 |\n",
        " |-  |  |  |  |  |\n",
        "GRU model | 01 | 0.3401 | 0.3579 | - |\n",
        " | 02 | 0.3201 | 0.3314 | - |\n",
        "  | - | - | - | 0.3374 | 0.0138 |\n",
        "  | - |  |  |  |  |\n",
        "CNN model | 01 |0.5943 | 0.5807 | - |\n",
        " | 02 | 0.5480 | 0.5817 | - |\n",
        " | - | - | - | 0.5812 | 0.0005 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF3AhBPVDqV-"
      },
      "source": [
        "Model name  | Kaggle Score\n",
        "-------------------|------------------|\n",
        "Standard Baseline | 0.21 |\n",
        "Deep Model | 0.42 |\n",
        "GRU Model | 0.36 |\n",
        "CNN Model | 0.59 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzqNfmDY6g-t"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Overall, the model recommended for this task is the CNN model. This is because it had the best performance, without overfitting - and this allows the results to be generalisable. When entered into Kaggle, the CNN model performed well, with an accuracy of 0.59. This is higher than the other three models: the SGD model had a score of 0.21, the deep model had a score of 0.42, and the multi-attention GRU model had a score of 0.36.  \n",
        "\n",
        "One recommendation for the future would be to ensure investment in consistent GPU. Google colab was used for this project, which has a limited amount of GPU for each user. As such, once that allotted amount ran out, all models took significantly longer to run. Another method to improve the model would be to add not only more images to the dataset, but also increase the number of emotions used as categories, to further improve both accuracy and range (and therefore generalisability of the model).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjTBAjH66vIO"
      },
      "source": [
        "## References\n",
        "\n",
        "Smith, L.N. (2018) A disciplined approach to neural network hyperparameters: Part 1 - - learning rate, batch size, momentum, and weight decay. US Naval Research Laboratory. Available at: https://arxiv.org/abs/1803.09820v2.\n",
        "\n",
        "\n",
        "Li, B., Guo, Y., Yang, J., Wang, L., Wang, Y. and An, W. (2021), ‘Gated Recurrent Multiattention Network for Remote Sensing Image Classification’, IEEE Transactions on Geoscience and Remote Sensing, pp.1-13. https://www.researchgate.net/figure/Illustration-of-the-proposed-gated-recurrent-multiattention-network-a-Multiscale_fig2_353467605.\n",
        "\n",
        "\n",
        "Heaton, J. (2008) Introduction to Neural Networks for Java: 2nd edition. Available at: https://dl.acm.org/doi/10.5555/1502373. (Accessed: 06 April 2024).\n",
        "\n",
        "\n",
        "Kim, S. (2021) ‘Demystifying Batch Normalization vs Drop out’, Medium, 11 October.  Available at: https://skirene.medium.com/demystifying-batch-normalization-vs-drop-out-1c8310d9b516#:~:text=Dropout%2C%20on%20the%20other%20hand,creates%20disharmony%20between%20those%20two.\n",
        "\n",
        "\n",
        "Géron, A. (2019) Hands-On Machine Learning with Scikit-learn, Keras, and TensorFlow. 2nd edn. California: O’Reilly.\n",
        "\n",
        "\n",
        "Muraina, I. (2022) ‘Ideal dataset splitting ratios in machine learning algorithms: general concerns for data scientists and data analysts’. Available at: https://www.researchgate.net/profile/Ismail-Muraina/publication/358284895_IDEAL_DATASET_SPLITTING_RATIOS_IN_MACHINE_LEARNING_ALGORITHMS_GENERAL_CONCERNS_FOR_DATA_SCIENTISTS_AND_DATA_ANALYSTS/links/61fb97e711a1090a79cc1a8b/IDEAL-DATASET-SPLITTING-RATIOS-IN-MACHINE-LEARNING-ALGORITHMS-GENERAL-CONCERNS-FOR-DATA-SCIENTISTS-AND-DATA-ANALYSTS.pdf.\n",
        "\n",
        "\n",
        "Chauhan, R., Ghanshala, K., Joshi, R.C. (2018) ‘Convolutional Neural Network (CNN) for Image Detection and Recognition’. 2018 First International Conference on Secure Cyber COmputing and Communication. https://doi.org/10.1109/ICSCCC.2018.8703316."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sKhXocJEBjEo",
        "jchDNqUjBqhN",
        "04X4sH3QCPoh",
        "tzO90uEnDHx4",
        "g1vJsWlbDqNW",
        "tvu_D9HKEqiD",
        "4TCVXJ5NExbj",
        "w3KeRt9iE1ws",
        "MvVPCBU8E2eX",
        "2GBr7a7UE3fx",
        "jrXtGnI_E38i",
        "Uilv6JICE4WD",
        "Ic2Y4IlePCoi",
        "uQyryjYaPCoj",
        "DgZGOGqUPCoj",
        "pWRVOBNQPCok",
        "74oD5PxbPCol",
        "uY6V3wVcPCom",
        "D-fmeAHv40Lt",
        "FUrH9QbLGMDk"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
